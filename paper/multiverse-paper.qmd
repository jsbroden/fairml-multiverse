---
format: acm-pdf
# use keep-tex to cause quarto to generate a .tex file
keep-tex: true
bibliography: bibliography.bib
title: "One Model Many Scores: Using Multiverse Analysis to Prevent Fairness Hacking and Evaluate the Influence of Model Design Decisions"
author:
  - name: Jan Simson
    email: jan.simson@lmu.de
    affiliation:
      name: LMU Munich and Munich Center for Machine Learning (MCML)
      city: Munich
      country: Germany
  - name: Florian Pfisterer
    affiliation:
      name: LMU Munich
      city: Munich
      country: Germany
  - name: Christoph Kern
    affiliation:
      name: LMU Munich and Munich Center for Machine Learning (MCML)
      city: Munich
      country: Germany
# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  # anonymous: true

  # comment this out to build a draft version
  final: true

  # comment this out to specify detailed document options
  # acmart-options: nonacm

  # acm preamble information
  copyright-year: 2024
  acm-year: 2024
  copyright: acmcopyright
  doi: "10.1145/3630106.3658974"
  conference-acronym: "FAccT '24"
  conference-name: "The 2024 ACM Conference on Fairness, Accountability, and Transparency"
  conference-date: "June 3--6, 2024"
  conference-location: "Rio de Janeiro, Brazil"
  # price: "15.00"
  isbn: "979-8-4007-0450-5/24/06"

  # if present, replaces the list of authors in the page header.
  # shortauthors: Simson et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
    \begin{CCSXML}
    <ccs2012>
      <concept>
          <concept_id>10003456.10010927</concept_id>
          <concept_desc>Social and professional topics~User characteristics</concept_desc>
          <concept_significance>500</concept_significance>
          </concept>
      <concept>
          <concept_id>10010147.10010257</concept_id>
          <concept_desc>Computing methodologies~Machine learning</concept_desc>
          <concept_significance>500</concept_significance>
          </concept>
    </ccs2012>
    \end{CCSXML}

    \ccsdesc[500]{Social and professional topics~User characteristics}
    \ccsdesc[500]{Computing methodologies~Machine learning}

  keywords:
    - algorithmic fairness
    - multiverse analysis
    - automated decision making
    - robustness
    - reliable machine learning
abstract: |
  A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. The downstream effects of ADM systems critically depend on the decisions made during a systems' design, implementation, and evaluation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these decisions are made implicitly, without knowing exactly how they will influence the final system. To study this issue, we draw on insights from the field of psychology and introduce the method of multiverse analysis for algorithmic fairness. In our proposed method, we turn implicit decisions during design and evaluation into explicit ones and demonstrate their fairness implications. By combining decisions, we create a grid of all possible "universes" of decision combinations. For each of these universes, we compute metrics of fairness and performance. Using the resulting dataset, one can investigate the variability and robustness of fairness scores and see how and which decisions impact fairness. We demonstrate how multiverse analyses can be used to better understand fairness implications of design and evaluation decisions using an exemplary case study of predicting public health care coverage for vulnerable populations. Our results highlight how decisions regarding the evaluation of a system can lead to vastly different fairness metrics for the same model. This is problematic, as a nefarious actor could optimise or "hack" a fairness metric to portray a discriminating model as fair merely by changing how it is evaluated. We illustrate how a multiverse analysis can help to address this issue.
  
execute:
  echo: false
  warning: false
---

```{r setup}
suppressPackageStartupMessages(library(tidyverse))
library(ggpubr)
library(patchwork)
data_dir <- file.path("data")

source("../R/helpers.R")
```

<!-- \thanks{This paper is supported by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research.} -->

# Introduction

Across the world, more and more decisions are being made with the
support of machine learning (ML) and algorithms; so called algorithmic
decision making (ADM). Examples of such systems can be found in finance
for loan approvals \citep{mukerjee2002}, the labor market for hiring
decisions or filtering resumes \citep{faliagka2012}, and the criminal
justice system to assess risks of recidivism \citep{angwin2016}. While
these systems are promising when designed well, raising hopes of
more accurate and objective decisions, their impact can be quite the
opposite when designed incorrectly. There are many examples of ADM systems
discriminating against people \citep{mehrabi2021}. One prominent example
was the \emph{robodebt} system, where the Australian government used an
algorithm to detect potential social security overpayments. Due to
serious flaws in the design of the system, it often overestimated debts
and put the burden on the accused to prove the contrary
\citep{henriques-gomes2023}. Other examples include the Dutch childcare
benefits system using an ADM system that was much more likely to accuse
immigrants of having committed fraud \citep{amnestyinternational2021}.

These fairness problems often occur because algorithms replicate biases
in the underlying training data. However, biases can also be amplified
throughout the machine learning pipeline depending on how exactly data
is processed and turned into outputs \citep{kern, rodolfa2020}.
Unfortunately, no silver bullet exists to prevent biases
in the machine learning pipeline \citep{agrawal} and legislation usually
provides little guidance. Understanding how modeling decisions interact
with fairness is therefore a prerequisite for effectively mitigating
unintended outcomes in practice. A systematic mapping of design
decisions to fairness outcomes can critically guide the model selection
process, as multiple models may achieve similar accuracy, but can
considerably differ in their fairness properties \citep{black2022}. Alarmingly, we demonstrate how the evaluation of the same model can
be modified to achieve large variability in a fairness metric,
potentially allowing the \emph{hacking} of fairness metrics. Related issues
regarding the hacking or washing of fairness metrics have recently been raised
in fair ML research \citep{meding2024fairness, aivodji2019fairwashing}.
As a result, preventing algorithms from introducing, reinforcing or hiding
biases requires careful study and evaluation of the -- often implicit --
decisions made while designing and evaluating a machine learning system.
To address this objective in a systematic and efficient way, we
introduce the method of multiverse analysis for algorithmic fairness.
Multiverse analyses were introduced to psychology with the intent to
improve reproducibility and create more robust research
\citep{steegen2016}. We adapt this methodology across domains to work in
the context of machine learning with a focus on evaluating metrics of
algorithmic fairness. We present two variations of this method
demonstrating its usefulness: (1) as a guidance during the design of the
model and preprocessing pipeline and (2) as an estimator of robustness
of a fairness metric and to protect against fairness hacking.

![**Steps to conduct a multiverse analysis for algorithmic fairness.** Steps 1 - 4 apply to multiverse analyses in general, whereas steps 5 - 6 are unique to larger multiverse analyses for algorithmic fairness.](figures/method_explanation.pdf){#fig-method-explanation fig-env="figure*" width="6in" height="4.04in"}

In the following, we present a generalizable approach of using
multiverse analysis to estimate the effect of decisions during the
design and evaluation of a machine learning or ADM system on fairness
outcomes. Using a case study of predicting public health coverage in US
census data we demonstrate how design decisions can be better understood
and fairness hacking can be addressed. We provide modular source code to
allow streamlined adaptation of the proposed method in other use cases
and contexts.

## Multiverse Analysis

Multiverse analyses were first introduced in psychology by
\citet{steegen2016} in response to the reproducibility crisis affecting
the field \citep{opensciencecollaboration2015}. The goal of this
analysis type is to investigate the invariance of results to
researchers' analysis decisions. Specifically, when analyzing a dataset,
researchers make many implicit and explicit choices \citep{simmons2011},
often without the option of confirming whether a choice is correct or
incorrect. This leads to many plausible scenarios when analyzing data,
as one traverses a \emph{garden of forking paths} \citep{gelman2014},
where each fork corresponds to a decision. The multitude of these
scenarios becomes especially evident when multiple researchers analyze
the same data, coming to staggeringly different results
\citep{breznau2022}.

Multiverse analysis focuses on the preprocessing steps applied to a
dataset: Steps such as selecting the observations and predictor
variables to include in a dataset or scaling and binning their values.
Based on the different decisions made and paths taken when
preprocessing a dataset, analysts will end up with one of many possible
datasets for the actual analysis. In a multiverse analysis, the goal is
to make this variation explicit by using the complete grid of decisions
and their options to generate all plausible datasets. Using all
potential datasets, a multiverse analysis re-runs the analysis on each
of them to receive the distribution of results instead of a single
result point (Figure~\ref{fig-method-explanation}, Steps 1 - 3). We extend
this methodology to also examine the influence of variation in evaluation
and adapt it for the machine learning context with a special
focus on using it to generate insights on metrics of algorithmic
fairness.

In addition to multiverse analysis, a related type of analysis, called specification curve analysis \citep{simonsohn2020specification} emerged in the social sciences literature. Its goal is to assess the strength of an effect of interest under the different modelling decisions contained in the complete grid of possible decision combinations. Results are aggregated in a specification curve, a graph displaying the distribution of the effect size or coefficient of interest, yielding a single curve that allows assessing the robustness of a measured association across modelling decisions. In contrast, our approach is not only interested in the robustness, but we aim to also identify decisions that impact the resulting fairness metrics for further investigation.

## Multiverse Analysis for Algorithmic Fairness
In our proposed adaptation of multiverse analysis for algorithmic
fairness, one starts by compiling a list of all potentially relevant
decisions that are being made during the design and evaluation of a particular system.
We differentiate between different kinds of decisions in this context:
(1) decisions which are already made explicitly with a consideration of
their different options e.g.~choice of model and its hyperparameters,
and (2) decisions which are made explicitly, but without any
consideration for alternatives e.g.~log-transforming an income column
because it is common practice. In a multiverse analysis, the goal is to
turn both types of decisions into completely explicitly made decisions
and evaluate their impacts. There are also decisions which may initially
not even be considered as such e.g.~modifying classification cutoffs
post-hoc due to external constraints. Conducting a multiverse analysis
invites reflection on the modeling pipeline such that implicit decisions
may surface and are turned into explicit ones. One of the key
differences in the present analysis compared to a classic multiverse
analysis is that we will evaluate machine learning systems, whereas
classical multiverse analyses will typically evaluate the outcomes of
null-hypothesis-significance-tests (NHST) across analysis choices. While
many of the decision points apply to any machine learning system (e.g.,
choice of algorithm, how to preprocess certain variables,
cross-validation splits), many of them are also domain-specific (e.g.,
coding of certain variables, how to set classification thresholds, how
fairness is operationalized). We focus on decisions made during the
preprocessing of data, in line with the original approach of multiverse
analyses \citep{steegen2016}. We extend this approach to incorporate
decisions relevant to algorithmic fairness, particularly with regard to
protected attributes and the translation of predictions into real-world
actions or interventions. Similarly to a classical multiverse analysis,
we use the resulting \emph{garden of forking paths} to generate a grid
of all possible universes of decision combinations, the multiverse. For each of these universes, we compute the resulting fairness and performance metrics of the machine learning system and collect them as a data point. Based on the
resulting dataset of decision universes and corresponding fairness
scores, we evaluate how individual decisions influence the fairness
metric and explore the most important decisions in more detail
(Figure~\ref{fig-method-explanation}).

Another novelty in our approach is our introduction of two distinct
perspectives on multiverse analyses: One with a focus on preprocessing,
fostering the understanding of how decisions affect models in a fairness
context and a second, focusing on robust fairness evaluation of ML
systems and protecting against cherry picking of evaluation criteria.


### Related Research

Existing work has described the effects of specific preprocessing or modeling decisions in isolation, such as the influence of different imputation methods \citep{caton2022}, of the model architecture, and of hyperparameters \citep{dooley2024rethinking} on fairness in different contexts.
Multiverse analyses have also been used to model the performance distribution in hyperparameter-space \citep{bell}, but not yet to analyze algorithmic fairness. Research into model multiplicity has discovered multiple sources of arbitrariness that can influence model predictions and fairness: Random samples of a dataset can lead to different predictions on the individual level \citep{cooper2024arbitrariness,friedler2019comparative}, the selection of different target variables can strongly affect model fairness \citep{watson-daniels2023multitarget} and even the original sampling during the creation of a dataset can be considered arbitrary \citep{meyer2023dataset}.

In terms of manipulating fairness, prior work has demonstrated the possibility of generating surrogate models that show little dependence on protected features for unfair models, a process termed "fairwashing" \citep{aivodji2019fairwashing}. Under an assumption of "fairness through unawareness", these surrogate models could then be presented as fair models. This assumption is unrealistic in practice, however, as there are commonly proxy variables available for protected attributes \citep{barocas2023classification}. Recent parallel work has demonstrated a process of using completely different fairness metrics to then report only the one with the most optimal score in a process also termed "fairness hacking" \citep{meding2024fairness}. In this work, we demonstrate how there is no need to vary the chosen fairness metric itself, if one is willing to shift evaluation criteria in order to manipulate its scores. We believe both of these approaches are troublesome and fall under the term "fairness hacking". They closely mirror practices of varying evaluation criteria to achieve significant p-values, a practice commonly referred to as "p-hacking", which gave rise to the introduction of multiverse analysis in psychology in the first place \citep{simonsohn2014pcurve}.

The field of hyperparameter-optimization (HPO)
\citep{feurer2019, bischl2023} tries to optimize the process of tuning
machine learning model hyperparameters. This field typically focuses on
optimizing algorithm performance by employing efficient search
strategies that allow optimizing performance
without requiring the exploration of the complete hyperparameter space.
However, adaptive search patterns such as, e.g. Bayesian Optimization \citep{snoek2012practical},  usually focus on efficiently finding the optimal
configuration and yield non-i.i.d. optimization traces. This makes them unsuitable for assessing the influence and robustness of any particular decision as post-hoc analysis relies on representative, i.i.d. data. While algorithmic fairness is also explored in the context of HPO \citep{pfisterer-arxiv19a, perrone2021}, the focus is only on finding models with favourable performance-fairness trade-offs instead of understanding the effects of individual decisions or assessing overall robustness. Here, we draw on insights and methodology from the field of HPO, in particular the functional analysis of variance (FANOVA) \citep{hooker2007, hutter2014} to allow a more interpretable and efficient analysis of the results from the multiverse analysis. Our focus, however, is on uncovering and systematically exploring variation induced by the different decisions instead of finding the setting that optimizes fairness metrics.

## Case Study

We illustrate how multiverse analysis can enrich the machine learning
fairness toolkit using a case study of predicting public health
insurance coverage. Accurate and fair prediction of public health
insurance coverage in the United States is an important issue as access
to healthcare is quite expensive in the US, with the country spending
almost $16\%$ of its gross domestic product per capita on healthcare in 2020
\citep{ortiz-ospina2017}. Whether or not someone is covered by health
insurance can have large effects on their health and financial situation:
According to \citet{sommers2017}, people with insurance have better self-reported health, have more preventative doctor's appointments, improved depression outcomes, and fewer personal bankruptcies.

We implement our case study using the ACSPublicCoverage dataset \citep{ding2021}, with data from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) \citep{us2021understanding}. We use this particular dataset as it is rich enough
for us to implement a wide range of design decisions and because many
other well-established datasets used in the fairness literature suffer
from non-trivial quality issues \citep{ding2021, fabris2022, bao}: UCI
Adult \citep{kohavi1996}, the most popular dataset in the fairness
literature \citep{fabris2022}, uses an arbitrary threshold of \$50,000
to create a binary task of income prediction. This threshold has been
shown to greatly influence the accuracy of predictions in certain
groups, biasing measures of algorithmic fairness and threatening
external validity \citep{ding2021}. The ACSPublicCoverage dataset is one
of the datasets which have been specifically developed in response to
the issues in UCI Adult.

Here, we operationalize having public insurance coverage as being covered by either Medicare, Medicaid, Medical Assistance (or any kind of government-assistance plan for those with low incomes or a disability) or Veterans Affairs Health Care, following the official Guidance for Health Insurance Data Users from the US Census Bureau \citep{uscensusbureau}. In line with the original task setup by \citet{ding2021}, only individuals with an age below 65 years and a yearly income of less than \$30,000 are examined. Low-income households are also more likely to rely on public health insurance \citep{keisler-starkey2022}.

As there are no clear guidelines on how to set up an ADM system within
this context (as would be the case in heavily regulated contexts such as
credit scoring) one faces a multitude of decisions when designing a
solution for this task, each of which can govern how bias is fed into
the final system. A multiverse analysis for algorithmic fairness
requires developers to make these design decisions explicit and shows
their fairness implications in the present context.

```{r clean_names}
clean_decision_names <- function(decisions) {
  decisions %>% 
    str_remove_all(fixed("sett_")) %>% 
    str_replace_all("_", " ") %>% 
    str_to_title()
}
clean_option_names <- function(options) {
  options %>% 
    case_match(
      .,
      "drop-name_race_Some Other Race alone" ~ "drop-other",
      "race-all" ~ "separate",
      "drop-smallest_race_1" ~ "drop-smallest_1",
      "drop-smallest_race_2" ~ "drop-smallest_2",
      "keep-largest_race_2" ~ "keep-largest_2",
      "locality-whitest-only" ~ "locality-most-privileged",
      .default = .
    ) %>% 
    str_replace_all("_", "-")
}
```

```{r load_data}
df_agg_full <- vroom::vroom(file.path(data_dir, "df_agg_prepared.csv.gz"))

# Minor re-coding to match values to article language
df_agg_full <- df_agg_full %>% 
  mutate(
    sett_eval_fairness_grouping = sett_eval_fairness_grouping %>% 
      case_match("race-all" ~ "separate", .default = sett_eval_fairness_grouping)
  )

df_no_eval <- df_agg_full %>% 
  filter_non_eval()
```


# Methodology

## Fairness Metric

While our proposed analysis works with multiple different fairness
metrics, it requires one to choose a primary metric for analysis. For
the present case study we used \emph{equalized odds difference}
\citep{agarwal, hardt} as the primary fairness metric, as it quantifies
the degree to which a system's predictions are equally good across
different groups defined by a protected attribute. Equalized odds
require both the \emph{true positive rate} (TPR) and the \emph{false
positive rate} (FPR) of a system's predictions to be equal across all
groups of the protected attribute. Values of the \emph{equalized odds
difference} can range from $0$ to $1$. A value of $0$ corresponds
to a perfectly fair model according to the metric, whereas a value of
$1$ corresponds to a completely unfair model. We use the
implementation from the fairlearn package \citep{weerts2023} to calculate
the metric, where the differences in both the \emph{true positive rate}
and the \emph{false positive rate} are calculated and the larger of the
two is used as the metric. We consider \emph{race} as the protected
attribute in our case study given the persisting racial disparities in
various domains, including health outcomes, in the US
\citep{obermeyer2019} and matching the original task \citep{ding2021}.

## Decision Space

When conducting a multiverse analysis, the first step is the
identification of relevant and plausible decisions to be made. Based on
the literature on data science and machine learning workflows
\citep{kuhn2020, lequy2022} we identified five distinct categories to
structure and guide the identification of decisions: Data Selection,
Preprocessing, Modeling, Post-Hoc and Evaluation decisions (Table
\ref{tbl-decisions}). As there is a potentially infinite list of possible decisions to consider, the present list is not intended to be exhaustive, but rather to highlight the most common and important categories of decisions one may typically encounter when designing a machine learning or ADM system. We also deliberately set the focus on decisions where alternative options are typically not considered or ones that are not identified as decisions at all. When adapting the methodology to a new system, this list can serve as an inspiration, however, one must also consider the domain-specific decisions unique to each applied problem.

```{r}
#| label: tbl-decisions
#| tbl-cap: Overview of the typical decision categories, the actual decisions examined in the case study and their respective options used to construct the multiverse.
decision_categories <- list(
  `Data Selection` = c(
    "exclude_features",
    "exclude_subgroups"
  ),
  `Preprocessing` = c(
    "scale",
    "preprocess_age",
    "preprocess_income",
    "encode_categorical"
  ),
  `Modeling` = c(
    "model",
    "stratify_split"
  ),
  `Post-Hoc` = c(
    "cutoff"
  ),
  `Evaluation` = c(
    "eval_fairness_grouping",
    "eval_exclude_subgroups",
    "eval_on_subset"
  )
)
find_decision_category <- function(decisions) {
  for (category_name in names(decision_categories)) {
    category <- decision_categories[[category_name]]
    decisions[decisions %in% category] <- category_name
  }
  decisions
}
decisions_raw <- list(
  `exclude_features` = c(
    "none",
    "race",
    "sex",
    "race-sex"
  ),
  `exclude_subgroups` = c(
    "keep-all",
    "drop-smallest_race_1",
    "drop-smallest_race_2",
    "keep-largest_race_2",
    "drop-name_race_Some Other Race alone"
  ),
  `scale` = c("do-not-scale", "scale"),
  `preprocess_age` = c("none", "bins_10", "quantiles_3", "quantiles_4"),
  `preprocess_income` = c("none", "bins_10000", "quantiles_3", "quantiles_4"),
  `encode_categorical` = c("one-hot", "ordinal"),
  `model` = c("logreg", "rf", "gbm", "elasticnet"),
  `stratify_split` = c(
    "none",
    "target",
    "protected-attribute",
    "both"
  ),
  `cutoff` = c("raw_0.5", "quantile_0.1", "quantile_0.25"),
  `eval_fairness_grouping` = c("majority-minority", "race-all"),
  `eval_exclude_subgroups`= c("exclude-in-eval", "keep-in-eval"),
  `eval_on_subset`= c(
      "full",
      "locality-largest-only", "locality-whitest-only",
      "locality-city-la", "locality-city-sf",
      "exclude-military", "exclude-non-citizens"
  )
)

decisions <- decisions_raw  %>%  
  lapply(\(x) paste(
    "(", 1:length(x), ")~", clean_option_names(x),
    sep = "",
    collapse = "; "
  ))
n_options <- decisions_raw %>% 
  lapply(length)
n_decisions <- length(decisions)
n_eval_decisions <- sum(names(decisions) %>% str_detect("eval_"))

tibble(
  `Category` = names(decisions) %>%
   find_decision_category() %>%
   paste0("\\textit{", ., "}"),
  `Decision` = names(decisions) %>% 
    clean_decision_names(),
  # `No. of \\mbox{Options}` = as.integer(n_options),
  # ` ` = " ",
  `Options` = as.character(decisions)
) %>% 
  group_by(Category) %>% 
  mutate(Category = if_else(row_number() == 1, Category, "")) %>% 
  knitr::kable(escape = FALSE, booktabs = TRUE) %>% 
  kableExtra::column_spec(3, width = "9cm") %>% 
  kableExtra::add_header_above(
    c(" ", "Decisions and Options Examined in Case Study" = 2),
    italic = TRUE
  ) %>% 
  kableExtra::group_rows(
    "Decisions examined in Study 1",
    1, n_decisions - n_eval_decisions
  ) %>% 
  kableExtra::group_rows(
    "Decisions examined in Study 2",
    n_decisions - n_eval_decisions + 1, n_decisions
  )
```

### Study 1: Model Design Decisions

We consider 9 distinct and orthogonal design
decisions. Each of these decisions has two to five unique choice
options, leading to a total of $N=61440$ combinations of decisions or
universes. We consider decisions roughly in the order they would
be made during a typical analysis.

\hypertarget{excluding-variables-as-predictors-exclude-features}{%
\textbf{Excluding Variables as Predictors (Exclude
Features).}\label{excluding-variables-as-predictors-exclude-features}} Selecting features to train a model on presents a critical design
decision. In the ADM context, it can be required to exclude certain
protected features (such as sex/gender, race, ethnicity) as predictors
due to legal constraints when designing a machine learning system.
However, as prominently shown in various studies this does not
necessarily lead to increased fairness, as the protected attribute is
often correlated with other ("legitimate) features \citep{weerts}. We implement the following options for this decision in our case study:
(1) use all features as predictors (incl.~protected ones), (2) exclude
race, the protected attribute in the case study, (3) exclude sex, a
sensitive attribute and (4) exclude both race and sex from modelling.

\hypertarget{sec-exclude-subgroups}{%
\textbf{Excluding Subgroups of the Protected Attribute (Exclude
Subgroups).}\label{sec-exclude-subgroups}} When working with variables with an uneven distribution or very rare
categories one may focus only on the most common groups, dropping data
for smaller ones. This can be done to preserve the privacy of small
groups, due to unreliability in the data or out of convenience to allow for an easier model interpretation
downstream. However, the exclusion of subgroups of the population can
potentially be harmful, with discriminatory differences in downstream
model predictions. While we decided to include this practice as a
decision in our analysis to (1) raise awareness of the issue and (2)
represent the effects of the practice in our analysis, this should not
be taken as an endorsement of this practice. We try to capture the implications of this practice via the attribute
race. We therefore chose to include a decision of dropping certain groups from the training data based on their prevalence. Groups were \emph{not} dropped from the test data used for evaluation as part of this decision. We include six options for this decision, with the fraction of discarded data in brackets\footnote{Fractions of discarded training data are only reported for a non-stratified train-test split, as there are only \emph{very slight} differences in the fraction of discarded data based on stratification strategy.}: (1) to keep all groups ($0.00\%$), (2)
to drop the smallest group ($0.01\%$), (3) to drop the two smallest
groups ($0.33\%$), (4) to keep the two largest groups ($27.45\%$)
and (5) to drop the category "Some Other Race alone" specifically
($15.81\%$).

\hypertarget{scaling-of-continuous-variables-scale}{%
\textbf{Scaling of Continuous Variables
(Scale).}\label{scaling-of-continuous-variables-scale}} It is common to scale continuous variables during preprocessing,
centering them on a mean of $\mu = 0$ and standard deviation of
$\sigma = 1$ (also referred to as z-scaling). Scaling may be
particularly advisable if kernel-based learners are used as it typically
leads to improved performance for such models. We include two options for this decision: (1) to keep continuous
variables as they are and (2) to scale continuous variables.

\hypertarget{binning-of-continuous-variables-preprocess-age-preprocess-income}{%
\textbf{Binning of Continuous Variables (Preprocess Age, Preprocess
Income).}\label{binning-of-continuous-variables-preprocess-age-preprocess-income}} Another common practice is binning continuous variables, i.e., turning
continuous variables into ordinal variables with discrete categories.
The reasons to do this are plentiful: To deal with outliers, to address
privacy concerns, or for a more tangible interpretation to name a few. We provide two distinct and orthogonal decisions here on whether or how
to bin the variables $age$ and $income$. We include four options for
the variable $age$: (1) perform no binning, (2) bin into bins of size
$10$, (3) bin into three evenly sized quantiles, (4) bin into four
evenly sized quantiles. Likewise, we include four options for the
variable $income$: (1) perform no binning, (2) bin into bins of size
$10,000$, (3) bin into three evenly sized quantiles, (4) bin into four
evenly sized quantiles.

\hypertarget{encoding-of-categorical-variables-encode-categorical}{%
\textbf{Encoding of Categorical Variables (Encode
Categorical).}\label{encoding-of-categorical-variables-encode-categorical}} Another common preprocessing step includes transforming categorical
variables into a numerical format. When doing this, one typically has two
options: (1) One-hot (or dummy) coding each variable with $K$
categories into $K$ (or $K - 1$) new binary variables or (2)
ordinally encoding each variable by assigning an integer value from
$1$ to $K$ for each category. Ordinal encoding is only applicable,
however, for variables with a natural ordering. For all ordinal variables (including continuous variables that have been
binned), we include both options. Any variables without a natural
ordering are always one-hot coded.

\hypertarget{model-type-model}{%
\textbf{Model Type (Model).}\label{model-type-model}} A major choice when designing any statistical or machine learning system is which model type one decides to use. While there is a large number of potential models to explore here, we focused on the most commonly used ones in the context of ADM in the literature. We note that hyperparameter selection has shown to have an impact on fairness, but choose to focus on other choices, as HPO has already been studied elsewhere \citep{perrone2021}. We therefore support the following model types as options for this
decision: (1) logistic regression \citep{cox1958}, (2) random forest
\citep{ho1995}, (3) gradient boosting machine \citep{friedman2001}, and
(4) elastic net \citep{zou2005} trained with their default hyperparameters.

\hypertarget{stratification-of-train-test-split-stratify-split}{%
\textbf{Stratification of Train-Test Split (Stratify
Split).}\label{stratification-of-train-test-split-stratify-split}} Training and test sets are often created by simple random splitting of
the full dataset. It can be beneficial, however, to perform this split
conditional on certain groupings to ensure equal representation of all
labels within both the train and test sets. We include four options for this decision: (1) to not stratify at all,
using a completely random split instead, (2) to stratify using the
target variable (\emph{public coverage}), (3) to stratify using the
protected attribute (\emph{race}) and (4) to stratify using a
combination of both variables.

\hypertarget{cutoff-for-final-classification-cutoff}{%
\textbf{Cutoff for Final Classification
(Cutoff).}\label{cutoff-for-final-classification-cutoff}} At the end of the ML pipeline, the prediction models' (risk) scores can
be used to classify new observations based on a pre-specified
classification threshold. By default a threshold of $0.5$ would be
used with every score equal or above classified as $1$ (\emph{having
coverage}) and everything below as $0$ (\emph{not having coverage}).
Actual interventions, however, are often based on the ranked list of
scores such that (costly) interventions are targeted at the top $X$
percent with the highest risk. With real-world scenarios often coming
with resource-bound restrictions, one may for example only be able to
provide an intervention for, say, $10\%$ or $25\%$ of the most
in-need in the population. These real-world restrictions are typically
not taken into account in fairness evaluations, despite having
potentially devastating implications. We therefore also consider different cutoff values for the final
predictions of the system. We support the following options for this
decision: (1) use the default raw cutoff value of $0.5$, (2) only
treat the lowest $0.1$ quantile as \emph{not having coverage}, (2)
only treat the lowest $0.25$ quantile as \emph{not having coverage}.

### Study 2: Evaluation

We consider 3 distinct and orthogonal decisions, all focusing on evaluation only. Each decision has between 2 and 7 options each. Together these produce a total of $N = 28$ unique evaluation strategies for any given model, without modifying the model or its predictions.

\hypertarget{grouping-of-protected-attribute-fairness-grouping}{%
\textbf{Grouping of Protected Attribute (Fairness
Grouping).}\label{grouping-of-protected-attribute-fairness-grouping}} When working with a fairness metric, it is necessary to specify for
which groups of the protected attribute it is calculated. The present
case study uses \emph{race} as the protected attribute. For protected
attributes with more than two categories, however, multiple comparisons
can be computed. Depending on the application context one may, e.g.,
simplify these groups into the largest group (\emph{majority}) and all
other groups (\emph{minority})\footnote{\textbf{Majority group}: `White
  alone'; \textbf{Minority group(s)}: `Asian alone', `Two or More Races',
  `Some Other Race alone', `Black or African American alone', `American
  Indian alone', `Native Hawaiian and Other Pacific Islander alone',
  `American Indian and Alaska Native tribes specified; or American
  Indian or Alaska Native, not specified and no other races' and `Alaska
  Native alone'.}. An important note regarding this decision is that it changes how the
fairness metric is calculated: with two groups, the difference between
those two groups is calculated, however, with more than two groups all
possible differences between group-pairs are calculated and the largest
difference between them is used (the default behaviour in \citet{weerts2023}). Naturally, this has a strong influence on the fairness metric.
 We include two options for this decision: (1) The fairness metric is
computed between the \emph{majority} group and \emph{minority} group and
(2) the fairness metric is computed as the maximum of the metric as
computed between all groups of the protected attribute
(\emph{race}).

\hypertarget{exclusion-of-subgroups-during-evaluation-eval-exclude-subgroups}{%
\textbf{Exclusion of Subgroups during Evaluation (Eval Exclude
Subgroups).}\label{exclusion-of-subgroups-during-evaluation-eval-exclude-subgroups}} Similarly to how subgroups of the protected attribute may be excluded from the training data, they
may also be excluded from the test data used for evaluation, with
potentially even greater adverse impact. We examine the exclusion of the same subgroups as in the decision
\emph{Exclude Subgroups} in Study 1
(Section~\ref{sec-exclude-subgroups}) and vary whether or not subgroups
are also excluded from the test dataset. The same warnings raised for
that decision are even more relevant for this decision and we
\emph{strongly} discourage the exclusion of subgroups in any system.

\hypertarget{evaluation-using-a-subset-of-the-data-eval-on-subset}{%
\textbf{Evaluation using a Subset of the Data (Eval on Subset).}\label{evaluation-using-a-subset-of-the-data-eval-on-subset}} When assessing the fairness of a system, the evaluation may happen on only a subset of the eventual target population, for example because some populations may be easier to reach or because the model deployment context changes over time. While this practice is obviously not desirable, it may be necessary in certain situations due to real-world limitations in resources. An example of this is the popular COMPAS dataset \citep{angwin2016} which was constructed using only data from a single county (Broward County, Florida), as a larger-scale construction of such a dataset would not have been feasible. We examine the following options for this decision, to represent possible population subsets one may use for evaluation: (1) examining only the largest geographical region (in terms of sample size), (2) examining the geographical region with the largest fraction of the privileged group; examining only data from the counties of (3) Los Angeles or (4) San Francisco, (5) examining a subset of only non-military people (as former military status may affect healthcare status), (6) examining only U.S. citizens and (7) not examining any subset, but rather using the full test data for evaluation.

## Technology

Analyses were conducted using Python Version 3.8 \citep{vanrossum2009}
and pipenv \citep{pipenvmaintainerteam2017} for reproducibility. The
Python package \mbox{scikit-learn} \citep{pedregosa2011} was used for
preprocessing and fitting of models, pandas \citep{team2020} for
loading and modification of data, folktables \citep{ding2021} for
retrieval of data, fairlearn \citep{weerts2023} for computation of
fairness metrics, fANOVA \citep{hutter2014} for calculation of variable
importance and papermill \citep{contributors2017} for parameterized
computation of decision universes. This reproducible document was
generated using quarto \citep{allaire2022}, R \citep{rcoreteam2022}
Version 4.2, the R packages from the tidyverse \citep{wickham2019} and
ggpubr \citep{kassambara2023} for generation of figures. The source code
of the analyses and this publication is available at
\mbox{\url{https://github.com/reliable-ai/fairml-multiverse}}. We purposefully created source code in a modular fashion to allow for easy adoption of the multiverse method in other fair ML contexts. An interactive analysis of a subset of the results is available at \url{https://reliable-ai.github.io/fairml-multiverse/}.

# Results

## Study 1: Model Design Decisions

```{r results-helpers}
format_number <- function(x, digits = 3) {
  x %>% 
    round(digits = digits) %>% 
    format(nsmall = digits)
}
```

```{r overall_stats}
cor_f1_fairness <- cor(
  df_no_eval$perf_ovrl_f1,
  df_no_eval$fair_main_equalized_odds_difference
) %>% 
  round(digits = 3)
cor_accuraccy_fairness <- cor(
  df_no_eval$perf_ovrl_accuracy,
  df_no_eval$fair_main_equalized_odds_difference
) %>% 
  round(digits = 3)
range_f1 <- range(df_no_eval$perf_ovrl_f1) %>% 
  round(digits = 3)
range_accuracy <- range(df_no_eval$perf_ovrl_accuracy) %>% 
  round(digits = 3)
range_fairness <- range(df_no_eval$fair_main_equalized_odds_difference) %>% 
  round(digits = 3)
print_mean_fairness <- function(df, for_plot = FALSE) {
  val_mean <- df$fair_main_equalized_odds_difference %>%
    mean() %>% 
    format_number()
  val_sd <- df$fair_main_equalized_odds_difference %>%
    sd() %>% 
    format_number()
  
  if (for_plot) {
    return(val_mean)
  } else {
    return(paste0("$", "M = ", val_mean, "$", " ($", "SD = ", val_sd , "$)")) 
  }
}
```

The multiverse analysis examining the influence of model design decisions produced a total of $N = 61440$ values of the fairness metric in Study 1\footnote{In Study 1, we evaluated all models using the same strategy, namely not aggregating groups of the protected attribute, not excluding any subgroups during evaluation, and evaluating on the complete test set.\label{fn1eval}}. When examining the distribution of the fairness metric across the multiverse of decisions, the large variation of the fairness metric becomes apparent, with values spanning the entire possible range of the metric from $0$ to $1$
(Figure~\ref{fig-variance}). Overall performance of the resulting models
was moderate with $F_{1}$ scores between $0$ and $0.598$ and raw
accuracies between $0.419$ and $0.722$. Performance and the fairness
metric were only weakly correlated with a Pearson correlation of
$r = 0.149$ for $F_{1}$ scores and $r = 0.192$ for raw accuracy.
For the $F_{1}$ score, the majority of universes fell into a similar
range of performance, but exhibited large variation on the fairness
metric (Figure~\ref{fig-performance-fairness}), highlighting the
opportunity to optimize algorithmic fairness without sacrificing
performance in line with \citet{islam2021}. Raw accuracy exhibited
similar opportunities, varying largely based on the decision
\emph{Cutoff}, with three large clusters of similar performance
(Figure~\ref{fig-performance-fairness-acc} A). For balanced accuracy the
distribution of fairness and performance values was slightly more
complex, exhibiting a slight fairness-performance trade-off
(Figure~\ref{fig-performance-fairness-acc} B).

```{r}
#| label: fig-variance
#| fig-cap: "**Variation in the multiverse spans the entirety of possible values of the fairness metric.** Distribution of fairness metric (equalized odds difference) across universes. Lower values on the fairness metric indicate smaller *TPR* and *FPR* differences across groups."
#| fig-width: 6
#| fig-height: 2.8
gghistogram(
  df_no_eval,
  x = "fair_main_equalized_odds_difference",
  fill = "#E41A1C",
  add_density = F
) +
  theme(legend.position = "top") +
  labs(x = "Fairness Metric (Equalized Odds Difference)",
       y = "Count")
```

```{r}
#| label: fig-performance-fairness
#| fig-width: 6.5
#| fig-height: 4
#| fig-cap: "**Performance and fairness are largely unrelated with plateaus of low variance in performance, but high variance in fairness.** Distribution of overall performance as $F_{1}$ score and fairness metric (equalized odds difference) across all multiverses. Marginal histogram shows distribution of performance. A marginal histogram of the fairness metric can be seen in Figure~\\ref{fig-variance}, similar figures for raw and balanced accuracy can be seen in Figure~\\ref{fig-performance-fairness-acc}. An \\href{https://reliable-ai.github.io/fairml-multiverse/}{interactive version} of this figure is available."
fig_perf_main_rawacc <- df_no_eval %>% 
  ggplot(aes(
    x = fair_main_equalized_odds_difference,
    y = perf_ovrl_accuracy
  )) +
  geom_hex(bins = 50) +
  scale_fill_distiller(
     direction = 1,
     palette = "Purples"
  ) +
  theme_pubr(legend = "top") +
  theme(
    legend.text = element_text(size = 9),
    legend.justification='left',
    axis.title.x = element_blank()
  ) +
  labs(
    y = "Performance (Raw Accuracy)",
    fill = "Count"
  ) +
  # Annotations
  geom_curve(
    data = data.frame(x = 0.55, y = 0.59, xend = 0.38, yend = 0.62),
    mapping = aes(
      x = x,
      y = y,
      xend = xend,
      yend = yend
    ),
    angle = 78L,
    curvature = -0.41,
    arrow = arrow(30L, unit(0.1, "inches"), "last", "closed"),
    inherit.aes = FALSE,
    color = "darkgrey"
  ) +
  geom_text(
    data = data.frame(
      x = 0.57, y = 0.593,
      label = "Plateaus of similar performance\nwith large variation in fairness"
    ),
    mapping = aes(x = x, y = y, label = label),
    hjust = 0L,
    inherit.aes = FALSE,
    size = 3,
    color = "darkgrey"
  )
fig_perf_margin_rawacc <- gghistogram(
  df_no_eval %>% 
    mutate(
      sett_cutoff = clean_option_names(sett_cutoff)
    ),
  x = "perf_ovrl_accuracy",
  fill = "sett_cutoff",
  palette = "Set1"
) +
  coord_flip() +
  theme(
    axis.ticks.y = element_blank(),
    axis.line.y = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = 9, angle = 45, vjust = 1, hjust = 1),
    legend.text = element_text(size = 9)
  ) +
  theme(
    legend.background = element_blank(),
    legend.justification='right'
  ) +
  labs(
    fill = "Cutoff"
  )

fig_perf_main_balacc <- df_no_eval %>% 
  ggplot(aes(
    x = fair_main_equalized_odds_difference,
    y = `perf_ovrl_balanced accuracy`
  )) +
  geom_hex(bins = 50) +
  scale_fill_distiller(
     direction = 1,
     palette = "Greens"
  ) +
  theme_pubr(legend = "top") +
  theme(
    legend.text = element_text(size = 9)
  ) +
  labs(
    x = "Fairness Metric (Equalized Odds Difference)",
    y = "Performance (Balanced Accuracy)",
    fill = "Count"
  )
fig_perf_margin_balacc <- gghistogram(
  df_no_eval,
  x = "perf_ovrl_balanced accuracy",
  fill = "grey"
) +
  coord_flip() +
  theme(
    axis.ticks.y = element_blank(),
    axis.line.y = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = 9, angle = 45, vjust = 1, hjust = 1),
    legend.text = element_text(size = 9)
  ) +
  theme(
    legend.background = element_blank(),
    legend.justification='right'
  )
fig_perf_main_f1 <- df_no_eval %>% 
  ggplot(aes(
    x = fair_main_equalized_odds_difference,
    y = perf_ovrl_f1
  )) +
  geom_hex(bins = 50) +
  scale_fill_distiller(
     direction = 1,
     palette = "Reds"
  ) +
  theme_pubr(legend = "top") +
  theme(
    legend.text = element_text(size = 9)
  ) +
  labs(
    x = "Fairness Metric (Equalized Odds Difference)",
    y = "Performance (F1)",
    fill = "Count"
  ) +
  # Annotations
  geom_curve(
    data = data.frame(x = 0.887, y = 0.222, xend = 0.869, yend = 0.467),
    mapping = aes(
      x = x,
      y = y,
      xend = xend,
      yend = yend
    ),
    arrow = arrow(30L, unit(0.1, "inches"), "last", "closed"),
    inherit.aes = FALSE,
    color = "darkgrey"
  ) +
  geom_text(
    data = data.frame(
      x = 0.72, y = 0.177,
      label = "Pleateu with similar performance\nand high variation in fairness"
    ),
    mapping = aes(x = x, y = y, label = label),
    inherit.aes = FALSE,
    size = 3,
    color = "darkgrey"
  )
fig_perf_margin_f1 <- gghistogram(
  df_no_eval,
  x = "perf_ovrl_f1",
  fill = "grey"
) +
  coord_flip() +
  theme(
    axis.ticks.y = element_blank(),
    axis.line.y = element_blank(),
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.text.x = element_text(size = 9, angle = 45, vjust = 1, hjust = 1),
    legend.text = element_text(size = 9)
  ) +
  theme(
    legend.background = element_blank(),
    legend.justification='right'
  )

(
  fig_perf_main_f1 + fig_perf_margin_f1
) +
  plot_layout(widths = c(5, 1))
```

```{r load_varimp}
varimp_overall <- read_csv(
  file.path(data_dir, "fanova_importance_interactions-overall.csv"),
  show_col_types = F
) %>% 
  janitor::clean_names()
```

```{r}
not_all_na <- function(x) any(!is.na(x))

prepare_varimp_table <- . %>% 
  arrange(desc(individual_importance)) %>% 
  head(10) %>% 
  select(where(not_all_na)) %>% 
  select(
    starts_with("level"),
    individual_importance,
    individual_std
  ) %>% 
  mutate(
    # Remove sett_ prefix
    across(starts_with("level_"), ~ clean_decision_names(.x)),
    # Wrap all level columns with mathit (since formulas do weird stuff to f)
    across(starts_with("level_"), ~ if_else(
      is.na(.x),
      NA,
      paste0("\\mathit{", .x, "}")
    )),
    # Round values
    individual_importance = round(individual_importance, 3),
    individual_std = round(individual_std, 3)
  ) %>% 
  # Combine levels into one column
  unite("level", starts_with("level_"), sep = " x ", na.rm = T) %>% 
  mutate(
    # Generate nice interaction labels
    interaction_level = (str_count(level, fixed(" x ")) + 1),
    type = if_else(
      interaction_level > 1,
      paste0(interaction_level ,"-way int."),
      "main"
    ),
    # Format the level column to use nice math notation
    level = level %>%
      str_replace_all(fixed(" x "), " \\times ") %>%
      str_replace_all(fixed("_"), "\\:") %>% 
      paste0("$", .,"$")
  ) %>% 
  # Add proper column labels
  select(
    `Effect Type` = type,
    `Decision / Interaction of Decisions` = level,
    `Importance` = individual_importance,
    `Std. Deviation` = individual_std
  ) %>%
  knitr::kable(escape = FALSE, booktabs = TRUE)
```

```{r}
#| label: tbl-varimp
#| tbl-cap: The 10 most important decisions or decision interactions and their relative importance.
varimp_overall %>% prepare_varimp_table()
```

### Importance of Decisions

We conducted a FANOVA \citep{hooker2007} as described in
\citet{hutter2014} to assess the importance of decisions on the fairness
metric. This analysis decomposes the overall variance of the fairness
metric into the fractions which are explained by each decision. These
variance decompositions are used to assess the relative importance of
decisions. Moreover, the FANOVA also allows computing explained variance
for interactions of decisions. This is highly useful, as the overall
interaction space between decisions is quite large with $511$ possible
(interaction and main) effects.

Using the resulting importance values from the FANOVA, one can see which
decisions are associated with a high variation in fairness scores,
whether it be by themselves or in conjunction with others. This allows
assessing the most consequential decisions on a one-by-one case. Table~\ref{tbl-varimp} contains a ranked list of the most important decisions
and decision interactions in our case study alongside their respective
importance.

As can be seen in Table~\ref{tbl-varimp}, the most important decision is how the stratification of
the train-test split is performed. Moreover, the interaction of the
chosen cutoff value with the stratification strategy is highly
important, accounting for more than $30\%$ of the variance in the
fairness metric. It also becomes apparent that especially the
\emph{interactions} of decisions are relevant here, with all decisions
among the top 10 except the stratification and cutoff being interactions
rather than sole decisions.

```{r}
theme_fig_top_decisions <- function() {
  theme_pubr() +
  theme(legend.position = "top")
}
viz_one_sett <- function(data, setting, sort_by_metric = TRUE) {
  if (sort_by_metric) {
    # Sort data by metric
    data <- data %>% 
      mutate(
        across(
          all_of(setting),
          ~ reorder(.x, fair_main_equalized_odds_difference)
        )
      )
  }
  data %>% 
    ggviolin(
      x = setting,
      fill = setting,
      alpha = .7,
      y = "fair_main_equalized_odds_difference",
      add = "mean"
    ) +
    coord_flip() +
    scale_fill_brewer(type = "qual", palette = "Set1") +
    labs(
      x = clean_decision_names(setting),
      y = "Fairness Metric"
    ) +
    theme_fig_top_decisions() +
    theme(legend.position = "none")
}
```

```{r}
#| label: fig-top-decisions
#| fig-cap: "**The influence of decisions on the fairness metric can only be understood when examining interactions on top of individual decisions.** Visualization of the fairness metric depending on the three most important decision / decision combinations (from A - C by importance) and their respective options."
#| fig-width: 9
#| fig-height: 5
df_no_eval_cleaned <- df_no_eval %>% 
  mutate(
    across(starts_with("sett_"), ~ clean_option_names(.))
  )
manually_order_cutoff <- . %>% 
  mutate(sett_cutoff = factor(sett_cutoff, levels = c(
    "raw-0.5",
    "quantile-0.1",
    "quantile-0.25"
  )))
fig_no_eval_1 <- viz_one_sett(df_no_eval_cleaned, "sett_stratify_split")
fig_no_eval_2 <- df_no_eval_cleaned %>% 
  manually_order_cutoff() %>% 
  group_by(sett_cutoff, sett_stratify_split) %>% 
  summarise(
    fair_main_equalized_odds_difference = mean(
      fair_main_equalized_odds_difference
    ),
    label = fair_main_equalized_odds_difference %>% 
      round(2) %>% 
      format(nsmall = 2)
  ) %>% 
  ggplot(
    aes(
      # Reordering by fairness looks a bit odd here, so let's not do that
      x = sett_cutoff,
      y = sett_stratify_split %>% 
        reorder(fair_main_equalized_odds_difference),
      fill = fair_main_equalized_odds_difference,
    )
  ) +
    geom_tile(alpha = .75) +
    geom_text(aes(label=label), size = 3) +
    scale_fill_distiller(palette = "GnBu", direction = 1) +
    labs(
      x = clean_decision_names("sett_cutoff"),
      y = clean_decision_names("sett_stratify_split"),
      fill = "Fairness Metric"
    ) +
    theme_fig_top_decisions() +
    scale_x_discrete(guide = guide_axis(n.dodge = 2))
fig_no_eval_3 <- viz_one_sett(
  df_no_eval_cleaned %>% manually_order_cutoff(),
  "sett_cutoff",
  sort_by_metric = FALSE
)
# Combine into one plot
layout <- "
A#
AB
CB
C#
"
(
  fig_no_eval_1 +
  (fig_no_eval_2 + theme(legend.position = "right")) +
  fig_no_eval_3
) +
  plot_layout(design = layout) +
  plot_annotation(tag_levels = c('A'))
```

We analyzed the three most important decisions or decision-interactions to further illustrate the methodology and how one would explore the results of the analysis. The results also highlight why one should investigate the decisions in a detailed manner and not just pick the most-fair and highest-performing universe's model. The decisions \emph{Stratify Split}, \emph{Cutoff} and their interaction account for all three of the most important decisions. When examining the decision separately, it can be seen how stratifying by the target variable leads to noticeably lower fairness scores (Figure~\ref{fig-top-decisions} A, most important) and how the raw cutoff value of $0.5$ is suddenly not leading to the best fairness scores anymore (Figure~\ref{fig-top-decisions} B, third most important). The effects of both variables become most clear, however, when examining their interaction, which was identified as explaining almost as much variance as the most important decision. While using a cutoff value corresponding to the top $10\%$ quantile leads to the least fair model when stratifying by the target variable it surprisingly leads to the models with the best average fairness metric when using any other stratification strategy (Figure~\ref{fig-top-decisions} C, second most important).

As variation in random train-test splits can affect fairness and performance of machine learning models \citep{cooper2024arbitrariness,friedler2019comparative}, we repeated the complete multiverse analysis five times with different random seeds, achieving highly similar results regarding both the overall variation of the fairness metric (Figure~\ref{fig-var-replications}) and the relative importance of decisions (Figure~\ref{fig-cor-replications}).

### Scaling a Design Multiverse Analysis for Algorithmic Fairness

```{r varimp_partial}
#| cache: true

# Compute correlation of variable importances for a certain fraction (across multiple files)
compute_varimp_correlation <- function(varimp_type, cor_method, fraction) {
  varimp_dir <- file.path(
    data_dir, "partial_fanova", varimp_type, paste0("fraction-", fraction)
  )
  files <- list.files(varimp_dir)
  full_filepaths <- file.path(varimp_dir, files)
  
  if (length(files) == 0) {
    stop("No files in the directory!")
  }
  
  # Compute the correlatino for every file in the correct directory
  correlations_vector <- sapply(full_filepaths, function(filepath) {
    compute_single_varimp_correlation(varimp_type, cor_method, filepath)
  })
  
  correlations_vector
}
# Compute correlation of variable importance for a single file
compute_single_varimp_correlation <- function(varimp_type, cor_method, filepath) {
  varimp_full <- get(paste0("varimp_", varimp_type))
  varimp_partial <- read_csv(
    filepath,
    show_col_types = F
  ) %>% 
    janitor::clean_names()
  
  levels <- varimp_full %>% select(starts_with("level_")) %>% colnames()
  
  varimp_joined <- varimp_full %>%
    select(
      all_of(levels),
      full_importance = individual_importance
    ) %>% 
    full_join(
      varimp_partial %>% 
        select(
          all_of(levels),
          partial_importance = individual_importance
        ),
      by = levels
    )
  
  cor(
    varimp_joined$full_importance,
    varimp_joined$partial_importance,
    method = cor_method
  )
}
# All work and no play makes jack a dull boy
# Collect all variable importance correlations
correlations <- expand_grid(
  varimp_type = c("overall"),
  cor_method = c("pearson", "spearman"),
  sample_frac = c("0.01", "0.05", "0.1", "0.2") 
) %>% 
  rowwise() %>% 
  mutate(
    correlation = list(compute_varimp_correlation(
      varimp_type = varimp_type,
      cor_method = cor_method,
      fraction = sample_frac
    ))
  ) %>% 
  unnest(correlation)
n_correlations <- correlations %>%
  group_by(across(-correlation)) %>%
  count() %>%
  pull(n) %>%
  unique()
if (length(n_correlations) > 1) { stop("Different numbers of variable importances") }
# Printing / formatting helper functions
get_value_fmt <- function (value) {
  if (value == 1.0) {
    # Value rounds to 1
    return(" \\geq 0.999")
  } else {
    return(paste0(" =", format(value, nsmall = 3)))
  }
}
# Print a single variable importance correlation
get_cor <- function (type, method, frac) {
  correlations_filtered <- correlations %>%
    filter(varimp_type == type, cor_method == method, sample_frac == frac) %>%
    pull(correlation)
  final_cor_vals <- c(
    mean = correlations_filtered %>% mean(),
    sd = correlations_filtered %>% sd(),
    min = correlations_filtered %>% min(),
    max = correlations_filtered %>% max()
  ) %>% 
    round(digits = 3)
  cor_sign <- if (method == "spearman") "\\rho" else "r"
  cor_sign_bar <- paste0("\\bar{", cor_sign, "}")
  cor_name <- paste0(cor_sign_bar, "_{", frac * 100, "\\%}")
  
  cor_distribution <- paste0(
    "(",
    "$SD = ", final_cor_vals["sd"], "$",
    # ", ", "$", cor_sign, "_{min} ", get_value_fmt(final_cor_vals["min"]), "$",
    # ", ", "$", cor_sign, "_{max} ", get_value_fmt(final_cor_vals["max"]), "$",
    ")"
  )
  
  cor_val <- final_cor_vals["mean"]
  cor_main_val_text <- paste0("$", cor_name, get_value_fmt(cor_val), "$")
  
  return(paste(cor_main_val_text, cor_distribution))
}
```

Conducting a multiverse analysis can be computationally expensive.
Especially if the multiverse is particularly large or computational resources
are limited, it may not be possible to explore the complete grid of universes. To assess the feasibility of running the multiverse analysis on a
smaller subset of the grid, we also conducted the FANOVAs on different
subsamples of the collected \emph{multiverse} dataset. Specifically, we
ran the analysis on random subsets of $1\%$, $5\%$, $10\%$ and $20\%$ of the data and calculated the correlation of variance
decomposition or importance values with the FANOVA estimated on the full
multiverse dataset. The estimates of variance decomposition are highly
skewed, with a few highly important decisions and a very larger number
of very low-importance decisions. We therefore calculated both, the
Pearson correlation which is more sensitive to correlations of the more
important decisions and the Spearman rank-correlation which is also
sensitive to decisions with low importance estimates. To assess the
consistency of this approach we computed the FANOVA on each subsample `r n_correlations`
times and calculated the correlation with the results from the full
\emph{multiverse} dataset every time.

When calculating the Pearson correlation, the resulting mean correlation coefficient ranged from `r get_cor("overall", "pearson", .01)` at $1\%$ to `r get_cor("overall", "pearson", .2)` at $20\%$. Spearman rank-correlations were also high, but lower than the Pearson correlation coefficients and more inconsistent (@fig-correlations), which indicates that using sparse data to estimate the importance of decisions works well for the most important decisions and less-so to identify nuances between the less-important decisions. The resulting Spearman rank-correlation mean coefficients ranged from `r get_cor("overall", "spearman", .01)` at $1\%$ to `r get_cor("overall", "spearman", .2)` at $20\%$.

## Study 2: Evaluation

```{r prepare_eval_data}
# Either use full data as base
df_agg_eval <- df_agg_full
# or use a subset where we can have more evaluation options by smartly combining
# sett_eval_exclude_subgroups and sett_exclude_subgroups
# df_agg_eval <- df_agg_full %>% 
#   filter(
#     # Only look at universes where we exclude subgroups
#     # (since "don't exclude during eval" is equivalent w/ "don't drop any")
#     (sett_eval_exclude_subgroups == "exclude-in-eval") &
#     # Race MUST not be a feature, else we would end up with different models
#     (sett_exclude_features %in% c("race-sex", "race"))
#   ) %>% 
#   mutate(
#     # We can now treat sett_exclude_subgroups as sett_eval_exclude_subgroups
#     sett_eval_exclude_subgroups = sett_exclude_subgroups
#   )
```

```{r}
eval_variation <- df_agg_eval %>% 
  group_by(pick(
    starts_with("sett_"),
    -starts_with("sett_eval_")
  )) %>% 
  summarise(
    fair_mean = mean(fair_main_equalized_odds_difference),
    fair_min = min(fair_main_equalized_odds_difference),
    fair_max = max(fair_main_equalized_odds_difference),
    fair_spread = fair_max - fair_min,
    variance = var(fair_main_equalized_odds_difference),
    sd = sd(fair_main_equalized_odds_difference)
  ) %>% 
  ungroup() %>%
  arrange(desc(variance))
```

```{r}
create_settings_string <- function(settings) {
  option_names <- settings %>% 
    t() %>% 
    clean_option_names()
  decision_names <- colnames(settings) %>% 
    clean_decision_names()
  
  paste(
    decision_names,
    option_names,
    sep = " = "
  ) %>% 
    paste(collapse = ", ")
}
```

```{r eval-max-var}
# Find the settings (and corresponding data) with the maximum variance
max_var_settings <- eval_variation %>% 
  head(1) %>% 
  select(all_of(starts_with("sett_")))

# Apply max variance filter
df_max_var <- df_agg_eval
for (setting in names(max_var_settings)) {
  df_max_var <- df_max_var %>% 
    filter(.[[setting]] == max_var_settings[[setting]])
}

max_var_settings_string <- create_settings_string(max_var_settings)

rm(max_var_settings)
```

```{r eval-med-var}
# Find the settings (and corresponding data) with the median variance
med_var_settings <- eval_variation %>% 
  # Get the middle row of settings => median as it's already ordered
  slice(round(nrow(eval_variation) / 2)) %>% 
  select(all_of(starts_with("sett_")))

# Apply max variance filter
df_med_var <- df_agg_eval
for (setting in names(med_var_settings)) {
  df_med_var <- df_med_var %>% 
    filter(.[[setting]] == med_var_settings[[setting]])
}

med_var_settings_string <- create_settings_string(med_var_settings)

rm(med_var_settings)
```

By combining the different evaluation decisions we end up with $N = `r nrow(df_max_var)`$ possible evaluation strategies for any given model. We computed each of these for each of the universes from Study 1. This lead to a total of $N = `r nrow(df_agg_eval) %>% format(big.mark = ",")`$ values of the fairness metric with a mean value of $M = `r mean(eval_variation$fair_mean) %>% format_number()`$. Similar to Study 1, these fairness values exhibited a high degree of variation. However, variation stayed high, even when examining values for the *exact same model*. We observe a full spread of the fairness metric from $0$ to $1$ ($\Delta = 1$) for `r (mean(eval_variation$fair_spread == 1) * 100) %>% format_number(2)`% of the models, only by varying their evaluation. Alarmingly, we observe a spread of at least $\Delta \geq 0.9$ on the fairness metric for `r (mean(eval_variation$fair_spread >= 0.9) * 100) %>% format_number(2)`% of models. In the following we examine variation due to evaluation for a single model in more detail.

```{r}
#| label: prep-figs-eval-var

filter_study_1_equivalent <- . %>% 
  # Do the equivalent filtering to Study 1 here note, this is not exactly the
  # same in terms of code, but the outcome should be the same
  # (due to the way some decisions interact)
  filter(
    sett_eval_fairness_grouping == "separate" &
    sett_eval_exclude_subgroups == "keep-in-eval" &
    sett_eval_on_subset == "full"
  )

obs_max_var_study_1_eval <- df_max_var %>% 
  filter_study_1_equivalent() %>% 
  pull(fair_main_equalized_odds_difference)
obs_med_var_study_1_eval <- df_med_var %>% 
  filter_study_1_equivalent() %>% 
  pull(fair_main_equalized_odds_difference)

fig_eval_variance_max <- df_max_var %>% 
  ggplot(aes(
    x = fair_main_equalized_odds_difference
  )) +
  geom_histogram(fill = "#377EB8", color = "black", alpha = .8) +
  geom_vline(
    aes(
      xintercept = obs_max_var_study_1_eval
    ),
    linetype = "dashed",
    color = "#000000"
  ) +
  theme_pubr() +
  labs(
    x = "Fairness Metric",
    y = "Counts"
  )
fig_eval_variance_med <- df_med_var %>% 
  ggplot(aes(
    x = fair_main_equalized_odds_difference
  )) +
  geom_histogram(fill = "#377EB8", color = "black", alpha = .8) +
  geom_vline(
    aes(
      xintercept = obs_med_var_study_1_eval
    ),
    linetype = "dashed",
    color = "#000000"
  ) +
  theme_pubr() +
  labs(
    x = "Fairness Metric",
    y = "Counts"
  )
```

```{r}
#| label: prep-figs-eval-raw

fig_eval_raw_max <- df_max_var %>% 
  ggplot(aes(
    x = sett_eval_exclude_subgroups %>% 
      clean_option_names(),
    y = sett_eval_on_subset %>% 
      clean_option_names(),
    label = fair_main_equalized_odds_difference %>%
      round(digits = 2) %>% 
      format(nsmall = 2)
    ,
    fill = fair_main_equalized_odds_difference
  )) +
  geom_tile() +
  geom_text(aes(color = fair_main_equalized_odds_difference > 0.5)) +
  scale_color_manual(values = c("#000000", "#FFFFFF")) +
  guides(color = F) +
  facet_wrap(~ sett_eval_fairness_grouping) + 
  theme_minimal() +
  scale_fill_viridis_c(direction = -1) +
  labs(
    x = clean_decision_names("sett_eval_exclude_subgroups"),
    y = clean_decision_names("sett_eval_on_subset"),
    subtitle = clean_decision_names("sett_eval_fairness_grouping"),
    fill = "Fairness\nMetric"
  ) +
  theme(
    plot.subtitle = element_text(hjust = 0.5),
    axis.text.x = element_text(angle = 30, vjust = 1, hjust=1)
  )

fig_eval_raw_med <- df_med_var %>% 
  ggplot(aes(
    x = sett_eval_exclude_subgroups %>% 
      clean_option_names(),
    y = sett_eval_on_subset %>% 
      clean_option_names(),
    label = fair_main_equalized_odds_difference %>%
      round(digits = 2) %>% 
      format(nsmall = 2)
    ,
    fill = fair_main_equalized_odds_difference
  )) +
  geom_tile() +
  geom_text(aes(color = fair_main_equalized_odds_difference > 0.5)) +
  scale_color_manual(values = c("#000000", "#FFFFFF")) +
  guides(color = F) +
  facet_wrap(~ sett_eval_fairness_grouping) + 
  theme_minimal() +
  scale_fill_viridis_c(direction = -1) +
  labs(
    x = clean_decision_names("sett_eval_exclude_subgroups"),
    y = clean_decision_names("sett_eval_on_subset"),
    subtitle = clean_decision_names("sett_eval_fairness_grouping"),
    fill = "Fairness\nMetric"
  ) +
  theme(
    plot.subtitle = element_text(hjust = 0.5),
    axis.text.x = element_text(angle = 30, vjust = 1, hjust=1)
  )

```


```{r}
#| label: fig-eval-med
#| fig-width: 9
#| fig-height: 4
#| fig-cap: "**The fairness metric of the exact same model can be significantly altered by varying its evaluation strategy alone (A) and especially the interaction of different evaluation decisions leads to changes in the fairness metric (B).** Overall distribution (A) and raw values (B) of fairness metric (equalized odds difference) for a single model over different decisions regarding its evaluation. The dashed line in A corresponds to the evaluation strategy used in Study 1\\footref{fn1eval}. Both plots display scores for a model showing median variation, to see the same figure for the model with high variation see Figure~\\ref{fig-eval-max} in the Appendix. An \\href{https://reliable-ai.github.io/fairml-multiverse/}{interactive version} of A is available, allowing examination of the distribution for any model in the multiverse analysis."

(
  fig_eval_variance_med + fig_eval_raw_med
) +
  plot_annotation(tag_levels = c('A'))
```

We examined the variation of two individual models in more detail to illustrate the impact of evaluation decisions on algorithmic fairness
for a single model. We chose to illustrate our point with one model exhibiting a median degree of variance based on evaluation decisions and one exhibiting a high degree. Neither model resulted from a particularly extreme combination of options.\footnote{The options for the model with high variance are: `r max_var_settings_string`. The options for the model with median variance are: `r med_var_settings_string`.}

The overall distribution of the fairness metric alongside a detailed breakdown by decisions can be seen in Figure~\ref{fig-eval-med} for the model with median variation and Figure~\ref{fig-eval-max} for the model with high variation. Under the evaluation strategy used in Study 1, the chosen model with high variance would be considered highly unfair with a metric of $m_{EqOdds} = `r obs_max_var_study_1_eval %>% format_number()`$ and the model with median variance slightly fairer with $m_{EqOdds} = `r obs_med_var_study_1_eval %>% format_number()`$. However, as can be seen in Figure~\ref{fig-eval-med}, there exist ample opportunities to tweak the evaluation strategy to achieve significantly better scores on the fairness metric. Indeed, both models can achieve a perfect score of 0 on the fairness metric, only by varying how they are evaluated. Given that the models stay exactly the same, we consider this practice "fairness hacking".

An overview of how evaluation decisions affect the fairness metric across the complete multiverse can be seen in Figure~\ref{fig-eval-decisions}, illustrating how e.g. the fairness grouping can consistently mask disparate treatment of minority groups. 

# Discussion

We demonstrate how multiverse analysis for algorithmic fairness provides a useful new method for evaluating the robustness of machine learning and ADM systems with respect to decisions along the modeling pipeline and their implications for algorithmic fairness. We highlight the importance of making decisions during model design and evaluation explicitly rather than implicitly.

By applying this new methodology in a use case of predicting public health care coverage, we demonstrate the feasibility of this approach as well as how fairness metrics can be manipulated through evaluation strategies. We further show which decisions during model design affect fairness the most: Surprisingly, we see that the stratification strategy used for the train-test split has strong effects on the fairness metric. We also observe that the cutoff value used for making final decisions is important, a decision often implemented post-hoc after model deployment without consideration of fairness.

When interpreting the results from a multiverse analysis for algorithmic fairness, one should evaluate results with care and strictly avoid merely selecting the combination of decisions with the best fairness metric. Results should be seen as an indication of how susceptible the fairness of a model is to design decisions and which decisions warrant closer examination. Relative scores of decision importance should always be interpreted in light of the overall degree of observed variation. Results from the analysis can also be used to guide the search of new options for the most important decisions. Final choices regarding the design of the system should be made using a combination of empirical results from the multiverse analysis and practical as well as ethical considerations within the context of the use case. The main goal of a multiverse analysis for algorithmic fairness is to facilitate making educated and explicit decisions. We recommend including complete results from the analysis alongside the final system.

As we explored only a single use case, we do not make any generalizable claims regarding the importance of any particular decisions, beyond the fact that these decisions \emph{can} matter and are worth investigating. Another limitation of this case study is that we only examined nine design and three evaluation decisions, with many plausible alternative decisions which could have been examined in their place or additionally. As there is an infinite space of decisions one may consider, we decided to draw the line at these decisions for illustrative purposes. A successful adoption of multiverse analysis for algorithmic fairness in different use cases and reporting of results could help identify a more exhaustive list of the most important decisions across contexts. Potential concerns regarding the computational cost of conducting a multiverse analysis for algorithmic fairness are valid, but can be addressed as we demonstrate that important decisions are robustly detected even when exploring only $1\%$ of the full \emph{multiverse}.

There are varying degrees of conducting a multiverse analysis of algorithmic fairness, each providing unique value and requiring different amounts of computation: We believe there is already significant value in (1) merely thinking about (implicit) decisions taken during system design and the consideration of potential alternatives, (2) performing a multiverse analysis of a fixed model with different evaluation strategies as a computationally inexpensive option to provide more robust evaluations and combat fairness hacking, (3) conducting a partial multiverse analysis of a subset of the full multiverse (e.g. 1\%) and (4) an analysis of the full multiverse as the most thorough option.

We encourage the use of the method during the design of future machine learning or ADM systems and provide an overview of the most important areas of decisions to guide analysts when adapting multiverse analysis for algorithmic fairness in their own context. We further provide a non-exhaustive list of exemplary decisions to serve as inspiration to identify potentially relevant decisions and source code that makes adoption to different use cases easy. We posit that results from a multiverse analysis for algorithmic fairness can critically inform discussions between developers and stakeholders and advise joint reflections on the ultimate design of ADM systems. We further advocate for the use of multiverse analysis in fairness evaluations to understand the distribution of fairness scores that can be evoked by the same model under different evaluation scenarios and to reduce the risk of potential fairness hacking by transparently reporting the entirety of results.

\section*{Research Ethics and Social Impact}

\subsection*{Ethics Statement}

Our selection of preprocessing and evaluation decisions builds on common practices observed in machine learning publications. While some of these practices such as excluding minority groups in preprocessing and evaluation are highly questionable and should not be normalized, we decided to include them in our case study to highlight their fairness implications and stimulate critical reflection. We further decided that criticism of individual manuscripts which implement such practices would not add much utility to our work, while potentially leading to (limited) negative consequences for their authors. Therefore, we present the implications of such data practices without singling out individual manuscripts.

\subsection*{Positionality Statement}

All authors are affiliated with organizations from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) countries, in line with a common pattern in the fair ML research community \citep{septiandri2023weird}. This background inherently influenced the research practice of this study, including the case study and data that was chosen, which ultimately predetermined the design and evaluation decisions we focused on. We posit, however, that the proposed methodology can be applied in a wide range of contexts, tasks, and with various different data modalities and protected attributes.

\subsection*{Adverse Impact Statement}

We condemn potential misuses of our proposed method that contrast its objective of promoting transparency and reliability in machine learning practice and identified the following potential adverse impacts and misconceptions.

\begin{itemize}
    \item We do not interpret fairness as an optimization problem. A multiverse analysis allows to understand the \textit{variation of fairness scores} as a result of design decisions that researchers and developers might not have related to fairness in standard modeling practice and although fairness scores can imply real fairness they are only an indicator and not proof of fairness. While its results can inform discussions on sensible design decisions, the social impacts of an ADM system can only be understood by considering its specific implementation context and the interactions with the social environment in which it is placed. 
    \item A multiverse analysis critically depends on the careful identification of \textit{relevant design decisions}. While the decisions we examined in our case study may serve as a starting point, they do not present an exhaustive list by any means. Specifying a multiverse analysis requires researchers to carefully reflect on the data practices, processing and modeling decisions, embedded in their respective application context. 
    \item A multiverse analysis should not be used to search for the evaluation strategy which displays the best fairness score. On the contrary, it presents a tool whose usage can be requested by stakeholders to instead \textit{prevent selective reporting} and promote transparency by presenting the distribution of fairness scores across multiple evaluation schemes. It re-centers the discussion on how and for whom fairness metrics are computed, and acknowledges the susceptibility and instability of metrics to (small) changes in the evaluation protocol.
\end{itemize}

\begin{acks}

This work is supported by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research, the Munich Center for Machine Learning and the Federal Statistical Office of Germany.

\end{acks}

# References

::: {#refs}
:::

\newpage

\appendix

# Supplementary Figures

```{=tex}
\counterwithin{figure}{section}
\counterwithin{table}{section}
\renewcommand\thefigure{\thesection\arabic{figure}}
\renewcommand\thetable{\thesection\arabic{table}}
```
```{r}
#| label: fig-performance-fairness-acc
#| fig-width: 6.5
#| fig-height: 8
#| fig-pos: 'h'
#| fig-cap: "**Performance and fairness are largely unrelated with clusters of low variance in performance, but high variance in fairness.** Distribution of overall performance as raw (A) or balanced (B) accuracy and fairness metric (equalized odds difference) across all multiverses. Marginal histograms show distribution of performance for different options of the *Cutoff* decision in A and overall in B. A marginal histogram of the fairness metric can be seen in Figure~\ref{fig-variance}. This figure is analogous to Figure~\ref{fig-performance-fairness} in the main text."

# Align axis range of marginal plots
max_range <- max(
  layer_scales(fig_perf_margin_rawacc)$y$range$range,
  layer_scales(fig_perf_margin_balacc)$y$range$range
)
fig_perf_margin_rawacc <- fig_perf_margin_rawacc + ylim(NA, max_range)
fig_perf_margin_balacc <- fig_perf_margin_balacc + ylim(NA, max_range)

(
  (( fig_perf_main_rawacc + (fig_perf_margin_rawacc + plot_layout(tag_level="new")) ) +
  plot_layout(widths = c(5, 1))) /
  (( fig_perf_main_balacc + (fig_perf_margin_balacc + plot_layout(tag_level="new")) ) +
  plot_layout(widths = c(5, 1)))
) + plot_annotation(tag_levels = 'A')
```

```{r}
#| label: replications-data-prep

# From random.org
replication_seeds <- c(
  "9617311",
  "9490635",
  "7076729",
  "2411824",
  "108100"
)
replication_names <- LETTERS[1:length(replication_seeds)]
names(replication_names) <- replication_seeds

replication_to_title <- function(seed) {
  name <- replication_names[seed]
  paste0("Replication ", name,"\n(Seed: ", seed, ")")
}
```


```{r}
#| label: fig-var-replications
#| fig-cap: "**Overall variation in the multiverse is highly similar across different replications.** Distribution of fairness metric (equalized odds difference) across universes in five different replications alongside the results reported in the main body of the paper. Lower values on the fairness metric indicate smaller *TPR* and *FPR* differences across groups."
#| fig-width: 7
#| fig-height: 4.5

load_replication <- function(seed) {
  file_path <- file.path(
    data_dir,
    "replications",
    paste0("df_agg_replication-", seed, ".csv.gz")
  )
  
  vroom::vroom(file_path) %>% 
    mutate(
      replication_seed = seed,
      is_replication = T
    )
}

df_replications <- map_dfr(replication_seeds, load_replication)
df_main_replications <- bind_rows(
  df_no_eval %>% mutate(
    title = "Main Analysis",
    is_replication = F
  ),
  df_replications %>% 
    mutate(
      title = replication_to_title(replication_seed)
    )
)

gghistogram(
  df_main_replications,
  x = "fair_main_equalized_odds_difference",
  fill = "is_replication",
  add_density = F
) +
  facet_wrap(~ title, ) +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  theme(legend.position = "none") +
  theme(strip.background = element_blank()) +
  labs(x = "Fairness Metric (Equalized Odds Difference)",
       y = "Count")
```

```{r}
#| label: fig-cor-replications
#| fig-cap: "**Estimates of decision importance are similar across replications of the analysis.** Correlations of variance decomposition / importance estimates between the analysis reported in the main body of the paper and five replications. Pearson correlation coefficients are consistently higher than Spearman correlation coefficients, indicating better estimation of high-importance decisions. Dashed lines were inserted at 0.9 and 1.0 to indicate high correlation values."
#| fig-width: 7
#| fig-height: 3

replication_correlations <- expand_grid(
  varimp_type = c("overall"),
  cor_method = c("pearson", "spearman"),
  seed = replication_seeds
) %>% 
  rowwise() %>% 
  mutate(
    filepath = file.path(
      data_dir,
      "replications",
      paste0("fanova_importance_interactions-replication-", seed, ".csv")
    ),
    correlation = list(compute_single_varimp_correlation(
      varimp_type = varimp_type,
      cor_method = cor_method,
      filepath = filepath
    ))
  ) %>% 
  unnest(correlation)

replication_correlations %>% 
  mutate(
    cor_method = cor_method %>% 
      str_to_title()
  ) %>% 
  ggplot(
    aes(
      x = correlation,
      y = seed %>% replication_to_title() %>% fct_rev(),
      color = cor_method,
      group = paste(varimp_type, cor_method)
    )
  ) +
  geom_vline(xintercept = c(0.9, 1.0), linetype = "dashed", alpha = 0.25) +
  geom_point(aes(shape = cor_method)) +
  scale_color_brewer(type = "qual", palette = "Set1") +
  theme_pubr(legend = "right") +
  labs(
    x = "Correlation",
    y = " ",
    shape = "Correlation Method",
    color = "Correlation Method"
  ) +
  scale_x_continuous(limits = c(0, 1))
```

```{r}
#| label: fig-correlations
#| fig-cap: "**Conducting the analysis with smaller subsets of the complete multiverse leads to similar results.** Correlations of variance decomposition / importance estimates between full dataset and random subsets of different sizes. Random subsets were drawn 50 times with points corresponding to mean correlations and lines to +/- 1 standard deviation. Pearson correlation coefficients are consistently higher than Spearman correlation coefficients."
#| fig-width: 6
#| fig-height: 3
correlations %>% 
  mutate(
    varimp_type = varimp_type %>% 
      str_replace("majmin", "majority-minority"),
    cor_method = cor_method %>% 
      str_to_title()
  ) %>% 
  group_by(across(-correlation)) %>% 
  summarise(
    middle = mean(correlation),
    sd = sd(correlation),
    lower = middle - sd,
    upper = middle + sd
  ) %>% 
  ungroup() %>% 
  mutate(
    sample_frac = as.numeric(sample_frac),
  ) %>% 
  ggplot(
    aes(
      x = sample_frac,
      y = middle,
      color = cor_method,
      group = paste(varimp_type, cor_method)
    )
  ) +
  geom_line(aes(linetype = cor_method), alpha = 0.5) +
  geom_point(aes(shape = cor_method)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.005) +
  scale_x_continuous(
    labels = scales::percent,
    breaks = correlations$sample_frac %>% 
      unique() %>% 
      as.numeric()
  ) +
  scale_color_brewer(type = "qual", palette = "Set1") +
  scale_y_continuous(limits = c(0, 1)) +
  theme_pubr(legend = "right") +
  theme(legend.direction = "vertical") +
  labs(
    x = "Fraction of Multiverse Used for Analysis",
    y = "Correlation",
    linetype = "Correlation Method",
    shape = "Correlation Method",
    color = "Correlation Method"
  )
```

```{r}
#| label: fig-eval-max
#| fig-width: 9
#| fig-height: 4
#| fig-cap: "**Evaluation decisions can strongly interact in their effect on the fairness metric.** Overall distribution (A) and raw values (B) of the fairness metric for a single model exhibiting high variation over different decisions regarding its evaluation. This figure is analogous to Figure~\ref{fig-eval-med} in the main text."

(
  fig_eval_variance_max + fig_eval_raw_max
) +
  plot_annotation(tag_levels = c('A'))
```

```{r}
#| label: fig-eval-decisions
#| fig-width: 7
#| fig-height: 10
#| fig-cap: "**Despite strong interactions for the same model, evaluation decisions exhibit general tendencies in how they affect algorithmic fairness.** Distribution of the fairness metric for different evaluation decisions across the complete multiverse of design decisions from studies 1 and 2."

viz_one_sett_eval <- function(data, setting, sort_by_metric = TRUE) {
  if (sort_by_metric) {
    # Sort data by metric
    data <- data %>% 
      mutate(
        across(
          all_of(setting),
          ~ reorder(.x, fair_main_equalized_odds_difference)
        )
      )
  }
  data %>% 
    ggviolin(
      x = setting,
      fill = setting,
      alpha = .7,
      y = "fair_main_equalized_odds_difference",
      add = "mean"
    ) +
    coord_flip() +
    scale_fill_brewer(type = "qual", palette = "Set1") +
    labs(
      x = clean_decision_names(setting),
      y = "Fairness Metric"
    ) +
    theme(legend.position = "none") +
    ylim(0, 1)
}

fig_sett_eval_exclude_subgroups <- viz_one_sett_eval(df_agg_eval, "sett_eval_exclude_subgroups")
fig_sett_eval_on_subset <- viz_one_sett_eval(df_agg_eval, "sett_eval_on_subset")
fig_sett_eval_fairness_grouping <- viz_one_sett_eval(df_agg_eval, "sett_eval_fairness_grouping")

(
  fig_sett_eval_exclude_subgroups / fig_sett_eval_on_subset / fig_sett_eval_fairness_grouping
) + plot_annotation(tag_levels = c('A')) + plot_layout(heights = c(1, 2, 1))
```
